{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN38YfBw1eS2XCxq4ycNwts",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/putrinahampun/PreparationForTFDSExam/blob/main/ExerciseDicoding/SubmissionB/ProblemB4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H-eYCKGkNSow",
        "outputId": "ab303646-87d7-4dc7-9480-545c2ffbf28d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "56/56 [==============================] - 1s 6ms/step - loss: 1.7454 - accuracy: 0.2292 - val_loss: 1.6788 - val_accuracy: 0.2270\n",
            "Epoch 2/20\n",
            "56/56 [==============================] - 0s 3ms/step - loss: 1.6211 - accuracy: 0.2635 - val_loss: 1.5674 - val_accuracy: 0.3371\n",
            "Epoch 3/20\n",
            "56/56 [==============================] - 0s 4ms/step - loss: 1.5062 - accuracy: 0.4730 - val_loss: 1.4352 - val_accuracy: 0.5888\n",
            "Epoch 4/20\n",
            "56/56 [==============================] - 0s 4ms/step - loss: 1.3133 - accuracy: 0.6635 - val_loss: 1.1946 - val_accuracy: 0.8090\n",
            "Epoch 5/20\n",
            "56/56 [==============================] - 0s 3ms/step - loss: 1.0240 - accuracy: 0.8281 - val_loss: 0.9120 - val_accuracy: 0.8404\n",
            "Epoch 6/20\n",
            "56/56 [==============================] - 0s 4ms/step - loss: 0.7369 - accuracy: 0.8972 - val_loss: 0.6658 - val_accuracy: 0.8989\n",
            "Epoch 7/20\n",
            "56/56 [==============================] - 0s 5ms/step - loss: 0.5233 - accuracy: 0.9258 - val_loss: 0.5083 - val_accuracy: 0.9079\n",
            "Epoch 8/20\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.3844 - accuracy: 0.9427 - val_loss: 0.4213 - val_accuracy: 0.9191\n",
            "Epoch 9/20\n",
            "56/56 [==============================] - 0s 5ms/step - loss: 0.2947 - accuracy: 0.9545 - val_loss: 0.3602 - val_accuracy: 0.9236\n",
            "Epoch 10/20\n",
            "56/56 [==============================] - 0s 5ms/step - loss: 0.2348 - accuracy: 0.9579 - val_loss: 0.3223 - val_accuracy: 0.9236\n",
            "Epoch 11/20\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.1907 - accuracy: 0.9657 - val_loss: 0.3044 - val_accuracy: 0.9236\n",
            "Epoch 12/20\n",
            "56/56 [==============================] - 0s 5ms/step - loss: 0.1585 - accuracy: 0.9719 - val_loss: 0.2826 - val_accuracy: 0.9213\n",
            "Epoch 13/20\n",
            "56/56 [==============================] - 0s 5ms/step - loss: 0.1338 - accuracy: 0.9758 - val_loss: 0.2661 - val_accuracy: 0.9326\n",
            "Epoch 14/20\n",
            "56/56 [==============================] - 0s 5ms/step - loss: 0.1148 - accuracy: 0.9792 - val_loss: 0.2541 - val_accuracy: 0.9326\n",
            "Epoch 15/20\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.0976 - accuracy: 0.9848 - val_loss: 0.2430 - val_accuracy: 0.9326\n",
            "Epoch 16/20\n",
            "56/56 [==============================] - 0s 3ms/step - loss: 0.0840 - accuracy: 0.9876 - val_loss: 0.2374 - val_accuracy: 0.9326\n",
            "Epoch 17/20\n",
            "56/56 [==============================] - 0s 3ms/step - loss: 0.0721 - accuracy: 0.9916 - val_loss: 0.2332 - val_accuracy: 0.9281\n",
            "Epoch 18/20\n",
            "56/56 [==============================] - 0s 4ms/step - loss: 0.0629 - accuracy: 0.9933 - val_loss: 0.2270 - val_accuracy: 0.9326\n",
            "Epoch 19/20\n",
            "56/56 [==============================] - 0s 4ms/step - loss: 0.0546 - accuracy: 0.9944 - val_loss: 0.2278 - val_accuracy: 0.9348\n",
            "Epoch 20/20\n",
            "56/56 [==============================] - 0s 4ms/step - loss: 0.0476 - accuracy: 0.9961 - val_loss: 0.2238 - val_accuracy: 0.9348\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        }
      ],
      "source": [
        "# ===================================================================================================\n",
        "# PROBLEM B4\n",
        "#\n",
        "# Build and train a classifier for the BBC-text dataset.\n",
        "# This is a multiclass classification problem.\n",
        "# Do not use lambda layers in your model.\n",
        "#\n",
        "# The dataset used in this problem is originally published in: http://mlg.ucd.ie/datasets/bbc.html.\n",
        "#\n",
        "# Desired accuracy and validation_accuracy > 91%\n",
        "# ===================================================================================================\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def solution_B4():\n",
        "\n",
        "    np.random.seed(42)\n",
        "    tf.random.set_seed(42)\n",
        "\n",
        "    bbc = pd.read_csv('https://github.com/dicodingacademy/assets/raw/main/Simulation/machine_learning/bbc-text.csv')\n",
        "\n",
        "    # DO NOT CHANGE THIS CODE\n",
        "    # Make sure you used all of these parameters or you can not pass this test\n",
        "    vocab_size = 1000\n",
        "    embedding_dim = 16\n",
        "    max_length = 120\n",
        "    trunc_type = 'post'\n",
        "    padding_type = 'post'\n",
        "    oov_tok = \"<OOV>\"\n",
        "    training_portion = .8\n",
        "\n",
        "    # YOUR CODE HERE\n",
        "    # Using \"shuffle=False\"\n",
        "\n",
        "    # Split the dataset into training and validation sets\n",
        "    training_sentences, validation_sentences = train_test_split(\n",
        "        #YOUR CODE HERE\n",
        "        bbc['text'],\n",
        "        train_size = training_portion,\n",
        "        shuffle=False\n",
        "    )\n",
        "    training_labels, validation_labels = train_test_split(\n",
        "        #YOUR CODE HERE\n",
        "        bbc['category'],\n",
        "        train_size = training_portion,\n",
        "        shuffle=False\n",
        "    )\n",
        "\n",
        "    # Fit your tokenizer with training data\n",
        "    tokenizer = Tokenizer(num_words=vocab_size, oov_token=oov_tok)\n",
        "    tokenizer.fit_on_texts(training_sentences)\n",
        "\n",
        "    # Convert sentences to sequences\n",
        "    training_sequences = tokenizer.texts_to_sequences(training_sentences)\n",
        "    training_padded = pad_sequences(training_sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n",
        "\n",
        "    validation_sequences = tokenizer.texts_to_sequences(validation_sentences)\n",
        "    validation_padded = pad_sequences(validation_sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n",
        "\n",
        "    # You can also use Tokenizer to encode your label.\n",
        "    # Convert labels to numerical values\n",
        "    label_tokenizer = Tokenizer()\n",
        "    label_tokenizer.fit_on_texts(bbc['category'])\n",
        "    training_label_seq = np.array(label_tokenizer.texts_to_sequences(training_labels))\n",
        "    validation_label_seq = np.array(label_tokenizer.texts_to_sequences(validation_labels))\n",
        "\n",
        "    # num_classes = len(label_tokenizer.word_index) + 1\n",
        "\n",
        "    model = tf.keras.Sequential([\n",
        "        tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n",
        "        tf.keras.layers.GlobalAveragePooling1D(),\n",
        "        tf.keras.layers.Dense(128, activation='relu'),\n",
        "        tf.keras.layers.Dense(6, activation='softmax')\n",
        "    ])\n",
        "\n",
        "    # Make sure you are using \"sparse_categorical_crossentropy\" as a loss function\n",
        "    model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "    # Train the model\n",
        "    history = model.fit(training_padded, training_label_seq, epochs=20,\n",
        "                        validation_data=(validation_padded, validation_label_seq))\n",
        "\n",
        "    return model\n",
        "\n",
        "    # The code below is to save your model as a .h5 file.\n",
        "    # It will be saved automatically in your Submission folder.\n",
        "if __name__ == '__main__':\n",
        "    # DO NOT CHANGE THIS CODE\n",
        "    model = solution_B4()\n",
        "    model.save(\"model_B4.h5\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "q3XPq57iUGd1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}